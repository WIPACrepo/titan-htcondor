#!/bin/bash
set -o errexit
set -o nounset
set -o xtrace
pool=$(readlink -f $1)
driver_script=$(readlink -f $2)
libexec=$(readlink -f $3)
condor_container=$(readlink -f $4)

set +o nounset
if [ -z "$PBS_NUM_NODES" ]; then
	PBS_NUM_NODES=0
fi
set -o nounset

echo Main script loading singularity
set +o xtrace
module load singularity
set -o xtrace

echo Main script starting pool initialization
mkdir -p $pool
cd $pool # o/w aprun may fail

test -f $pool/pool_is_ready && rm $pool/pool_is_ready
test -f $pool/pool_kill && rm $pool/pool_kill
test -f $pool/pool_nodes && rm $pool/pool_nodes
test -f $pool/pool_no_idle_jobs && rm $pool/pool_no_idle_jobs

archive=$pool/archive/$(date -u +%F_%T)
mkdir -p $archive
cp $pool/* $archive || true
test -d $pool/nodes && mv $pool/nodes $archive

sleep $((PBS_WALLTIME-60)) && echo "out of time" >> $pool/pool_kill &

if ((PBS_NUM_NODES == 0)); then
	singularity exec --pid $condor_container \
				$libexec/init-cm-node.sh $pool STARTD &
elif ((PBS_NUM_NODES == 1)); then
	singularity exec --pid $condor_container \
				$libexec/init-cm-node.sh $pool &
	aprun -D 1 singularity exec --pid $condor_container \
				$libexec/init-worker-node.sh $pool &
else
	aprun -D 1 -n 1 -d 16 \
			singularity exec --pid $condor_container \
					$libexec/init-cm-node.sh $pool STARTD &
	# Split up workers into multiple aprun "applications", to prevent the entire
	# pool from being killed sometimes if there is a problem with a worker (e.g.
	# running out of memory). Note that only 50 concurrent apruns are allowed, and
	# one of them is used for the central manager, so, in theorty, we can create
	# up to 49 aprun "apps". In practice, we have to create many fewer apps or we
	# will run into the 200 process limit (at least that's what I think happens).
	NUM_WORKER_APPS=20
	if ((PBS_NUM_NODES - 1 <= NUM_WORKER_APPS)); then
		for ((i = 1; i <= PBS_NUM_NODES - 1; i++)); do
			aprun -D 1 -n 1 -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
		done
	else
		jobs_per_aprun_quotient=$(((PBS_NUM_NODES - 1)/(NUM_WORKER_APPS - 1)))
		jobs_per_aprun_remainder=$(((PBS_NUM_NODES - 1)%(NUM_WORKER_APPS - 1)))
		for ((i = 1; i <= NUM_WORKER_APPS - 1; i++)); do
			aprun -D 1 -n $jobs_per_aprun_quotient -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
		done
		aprun -D 1 -n $jobs_per_aprun_remainder -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
	fi
fi

date -u >> $pool/main
date -u +%s >> $pool/main
env >> $pool/main
apstat -ar >> $pool/main

while ! test -f "$pool/pool_is_ready"; do
	echo Main script waiting for $pool/pool_is_ready
	sleep 5
done

ssh -p 2222 $(<$pool/cm_addr) $driver_script
ssh -p 2222 $(<$pool/cm_addr) $libexec/pool-shutdown-agent.sh $pool &

echo Main script finished initialization
set +o xtrace
while ! test -f "$pool/pool_kill"; do
	sleep 30
done

# vim:ft=sh
