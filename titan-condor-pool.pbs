#!/bin/bash
set -o errexit
set -o nounset
set -o xtrace
pool=$(readlink -f $1)
driver_script=$(readlink -f $2)
libexec=$(readlink -f $3)
condor_container=$(readlink -f $4)
echo Arguments: $pool $driver_script $libexec $condor_container
job_ts=$(date -u +%s.%F_%T)

set +o nounset
if [ -z "$PBS_NUM_NODES" ]; then
	echo Setting PBS_NUM_NODES to 0
	PBS_NUM_NODES=0
fi
set -o nounset

echo Main script loading singularity
set +o xtrace
module load singularity
set -o xtrace

echo Main script starting pool initialization
cd $pool # o/w aprun may fail

test -f $pool/pool_is_ready && rm $pool/pool_is_ready
test -f $pool/pool_kill && rm $pool/pool_kill
test -f $pool/pool_nodes && rm $pool/pool_nodes
test -f $pool/pool_no_idle_jobs && rm $pool/pool_no_idle_jobs
test -f $pool/pool_out_of_time && rm $pool/pool_out_of_time

archive=$pool/archive/$job_ts
mkdir -p $archive
cp $pool/* $archive || true
test -d $pool/nodes && mv $pool/nodes $archive

if ((PBS_NUM_NODES == 0)); then
	echo Starting pool in service node mode
	singularity exec --pid $condor_container \
				$libexec/init-cm-node.sh $pool STARTD &
elif ((PBS_NUM_NODES == 1)); then
	echo Starting pool in 1 worker node mode
	singularity exec --pid $condor_container \
				$libexec/init-cm-node.sh $pool &
	aprun singularity exec --pid $condor_container \
				$libexec/init-worker-node.sh $pool &
else
	echo Starting pool in multi-worker node mode
	aprun -n 1 -d 16 \
			singularity exec --pid $condor_container \
					$libexec/init-cm-node.sh $pool &
	sleep 5
	# Split up workers into multiple aprun "applications", to prevent the entire
	# pool from being killed sometimes if there is a problem with a worker (e.g.
	# running out of memory). Note that only 50 concurrent apruns are allowed, and
	# one of them is used for the central manager, so, in theorty, we can create
	# up to 49 aprun "apps". In practice, we have to create many fewer apps or we
	# will run into the 200 process limit (at least that's what I think happens).
	NUM_WORKER_APPS=20
	if ((PBS_NUM_NODES - 1 <= NUM_WORKER_APPS)); then
		echo Starting $((PBS_NUM_NODES - 1)) workers
		for i in $(seq $((PBS_NUM_NODES - 1))); do
			aprun -n 1 -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
		done
	else
		jobs_per_aprun_quotient=$(((PBS_NUM_NODES - 1)/(NUM_WORKER_APPS - 1)))
		jobs_per_aprun_remainder=$(((PBS_NUM_NODES - 1)%(NUM_WORKER_APPS - 1)))
		echo Starting $((NUM_WORKER_APPS - 1))x$jobs_per_aprun_quotient+jobs_per_aprun_remainder workers
		for i in $(seq $((NUM_WORKER_APPS - 1))); do
			aprun -n $jobs_per_aprun_quotient -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
			sleep 5
		done
		aprun -n $jobs_per_aprun_remainder -d 16 \
					singularity exec --pid $condor_container \
							$libexec/init-worker-node.sh $pool &
	fi
fi

main_log=$pool/main.$job_ts
date -u > $main_log
date -u +%s >> $main_log
env >> $main_log
ps aux | grep $USER >> $main_log
sleep 5 # give apruns time to launch or they won't show up in apstat
apstat -ar >> $main_log

while ! test -f "$pool/pool_is_ready"; do
	echo Main script waiting for $pool/pool_is_ready
	sleep 10
done

ssh -p 2222 $(<$pool/cm_addr) $driver_script
ssh -p 2222 $(<$pool/cm_addr) $libexec/pool-shutdown-agent.sh \
								$pool $((PBS_WALLTIME - 120)) &

echo Main script finished initialization
set +o xtrace
while ! test -f "$pool/pool_kill"; do
	sleep 20
done
echo Main script exiting

# vim:ft=sh
